{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Machine Learning and High Performance Computing\n",
        "\n",
        "---\n",
        "\n",
        "## 1. What is HPC?\n",
        "\n",
        "High Performance Computing (HPC) refers to the use of clusters or supercomputers composed of **many interconnected processors** that operate in parallel to perform calculations at very high speed.\n",
        "\n",
        "An HPC cluster consists of several computers that work together to run programs very fast. Each individual computer is called a **node**.\n",
        "\n",
        "By distributing tasks across multiple nodes and using high-speed networks for communication, HPC systems can handle large-scale simulations, data-intensive computations, and machine learning workloads that are infeasible on standard desktop or laptop computers.\n",
        "\n",
        "HPC is like having hundreds of computers (nodes) work together at once so big problems can be solved much faster than on a single laptop."
      ],
      "metadata": {
        "id": "4dPM7mujsQP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. HPC@UTD\n",
        "\n",
        "The **HPC team** at UT Dallas manages two main HPC clusters:\n",
        "\n",
        "- **Ganymede2**  \n",
        "  A computation condo HPC system. Ganymede2 assets are primarily owned by private researchers, the system has what are called “preempt” queues, which allow job submission from all Ganymede2 users.\n",
        "\n",
        "- **Juno**  \n",
        "  An HPC cluster available for demanding research computing workloads. Juno is available to faculty, students and staff working on research projects.\n",
        "\n",
        "For full documentation, see: [HPC@UTD](https://hpc.utdallas.edu/)\n"
      ],
      "metadata": {
        "id": "UzIZwngi9w6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting Started with Ganymede2\n",
        "\n",
        "### Step 1: Request an Account\n",
        "Apply here: [Account Request](https://hpc.utdallas.edu/services/)  \n",
        "Accounts are created by research groups. First the faculty PI applies and then the members of their research group apply.\n",
        "\n",
        "### Step 2: Set Up VPN\n",
        "Your computer must be connected to the the UTD campus network via\n",
        "- Wired Ethernet connection on campus, or\n",
        "- WiFi connection to CometNet, or\n",
        "- A VPN connection to campus **UT Dallas VPN**.  \n",
        "[VPN Instructions](https://atlas.utdallas.edu/TDClient/30/Portal/KB/ArticleDet?ID=152)\n",
        "\n",
        "### Step 3: Choose a Terminal Client\n",
        "\n",
        "To use Ganymede2 (or Juno), you will connect through the **command-line interface (CLI)**.  \n",
        "When you log in, you are given a **command prompt (shell)** where you type commands and see the output.  \n",
        "Accessing the shell requires a **terminal application**, and the choice depends on your operating system and preference.\n",
        "\n",
        "---\n",
        "\n",
        "**Linux**  \n",
        "- All Linux distributions have a built-in terminal.  \n",
        "- To open it, search for \"Terminal\" in your applications menu.  \n",
        "- The exact name may vary depending on your desktop environment, but it should always be available.  \n",
        "\n",
        "---\n",
        "\n",
        "**macOS**  \n",
        "- macOS comes with a built-in terminal application called **Terminal**.  \n",
        "- It may not appear in the Dock by default. To open it:  \n",
        "  - Use **LaunchPad** and search for \"Terminal\"  \n",
        "  - Or use **Spotlight Search** (Command + Space) → type \"terminal\" → hit Enter  \n",
        "- Alternative terminal applications (like [Ghostty](https://ghostty.org/docs)) provide extra features.\n",
        "\n",
        "---\n",
        "\n",
        "**Windows**  \n",
        "- Windows does not include a native UNIX-like terminal, but it has **Command Prompt** and **PowerShell**.  \n",
        "- These can connect to HPC clusters but are very limited in UNIX functionality.  \n",
        "- Better options include:  \n",
        "  - **MobaXterm**: provides a native-lik UNIX shell (mimic a real UNIX shell inside Windows)  with a package manager  \n",
        "  - **PuTTY**: lightweight tool for connecting to remote systems (no native shell)  \n",
        "  - **Windows Subsystem for Linux (WSL)**: creates a Linux environment with a full command-line interface  \n",
        "\n",
        "**Note:** While you may use Command Prompt or PowerShell, they are only sufficient for basic connectivity.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Connecting to the HPC System\n",
        "\n",
        "Before connecting, make sure:  \n",
        "1. You are on the **UTD campus network** or **UT Dallas VPN**.  \n",
        "2. You have a **terminal client** installed (macOS/Linux: Terminal; Windows: MobaXterm, PuTTY, or WSL).\n",
        "\n",
        "#### Option 1: Basic SSH Login\n",
        "\n",
        "The standard way to log in to a Linux system remotely is with Secure Shell (SSH). The format is:\n",
        "\n",
        "Run the ssh command inside a terminal, to connect to HPC, use the command\n",
        "\n",
        "```bash\n",
        "ssh <USERNAME>@<HOSTNAME>\n",
        "```\n",
        "\n",
        "- Replace <USERNAME> with your UT Dallas NetID\n",
        "- Replace <HOSTNAME> with the HPC system name (e.g., ganymede2.utdallas.edu)\n",
        "\n",
        "The terminal will prompt you for a password (NetID password).\n",
        "\n",
        "#### Option 2: Generate an SSH Key (Optional, Recommended)\n",
        "\n",
        "- Allows password-free login.\n",
        "\n",
        "In a terminal, run:\n",
        "\n",
        "```bash\n",
        "ssh-keygen -t ed25519\n",
        "```\n",
        "- Press **Enter** to save to default location (`~/.ssh/id_ed25519`).  \n",
        "- Optionally, set a passphrase (you’ll enter it twice).  \n",
        "\n",
        "This creates two files:  \n",
        "- Private key: `~/.ssh/id_ed25519` (keep secret!)  \n",
        "- Public key: `~/.ssh/id_ed25519.pub` (shareable)\n",
        "\n",
        "Then, copy Your Public Key to the HPC System\n",
        "\n",
        "```bash\n",
        "ssh-copy-id -i ~/.ssh/id_ed25519.pub user@host2\n",
        "```\n",
        "\n",
        "- Enter your NetID password once.  \n",
        "- You should see confirmation: `Number of key(s) added: 1`\n",
        "\n",
        "Log in with Your SSH Key\n",
        "\n",
        "```bash\n",
        "ssh <USERNAME>@<HOSTNAME>\n",
        "```\n",
        "- If you set a passphrase, enter it now. Otherwise, you’ll log in directly without your NetID password.  \n",
        "\n",
        "Detailed instructions can be found here: [UTD CIRC Connecting Guide](https://docs.circ.utdallas.edu/user-guide/intro-to-hpc/connecting.html). Please note that the HPC team is planning to retire this guide page, and a replacement is not yet available. For updates, please contact HPC@UTD.\n"
      ],
      "metadata": {
        "id": "ubEyZqY69uU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Once Logged In\n",
        "\n",
        "You should see a welcome message, current disk quotas and G2 command promp. You are now ready to use Ganymede2!\n",
        "\n",
        "Common commands include:\n",
        "\n",
        "Directories:\n",
        "\n",
        "- `pwd` → print/show current working directory\n",
        "- `cd` → change current directory\n",
        "- `mkdir` → make a new directory\n",
        "\n",
        "Editor:\n",
        "\n",
        "- `nano` → a full screen interactive editor to edit text files\n",
        "\n",
        "The module command:\n",
        "\n",
        "- `module avail` → Lists all available software modules on the system.\n",
        "- `module list`  → Displays the modules that are currently loaded in your session.\n",
        "- `module load <module name>` → Loads a specific module (e.g., python/3.8, R/4.4.1).  \n",
        "- `module unload <module name>` → Unloads a specific module\n",
        "- `module spider <module_name>` — Searches for a specific module and shows\n",
        "detailed information, including available versions and dependencies.  \n",
        "- `module keyword <keyword>` — Finds all modules whose names or descriptions contain the specified keyword.\n",
        "\n",
        "Check the available nodes:\n",
        "\n",
        "- `sinfo` → shows status of partitions/queues  \n",
        "- `scontrol show node c-05-11` → view detailed node information\n",
        "\n",
        "Job related:\n",
        "\n",
        "- `sbatch <script_name>` → submits a job script to the scheduler\n",
        "- `squeue -u <netID>` → lists running and waiting jobs  \n",
        "- `scontrol show job <job_id>` → shows detailed information about a specific job  \n",
        "- `scancel <job_id>` → cancel a submitted job"
      ],
      "metadata": {
        "id": "1EasAfEZ99a1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame for HPC node states\n",
        "data = {\n",
        "    \"STATE\": [\"idle\", \"alloc\", \"mix\", \"down\", \"drain\", \"comp\"],\n",
        "    \"Meaning\": [\n",
        "        \"The node is free and ready to run a job.\",\n",
        "        \"The node is fully allocated — all resources are in use.\",\n",
        "        \"The node is partially allocated — some resources are still available.\",\n",
        "        \"The node is offline (e.g., under maintenance or error).\",\n",
        "        \"The node is being drained of jobs and not accepting new ones.\",\n",
        "        \"The node is completing its current jobs before becoming idle.\"\n",
        "    ],\n",
        "    \"Indicator\": [\"✅ Available\", \"🚫 Busy\", \"⚙️ Partially Used\", \"🛠️ Unavailable\", \"🧹 Draining\", \"🕒 Finishing Up\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display nicely formatted table\n",
        "df.style.set_caption(\"HPC Node States (from sinfo)\") \\\n",
        "    .set_table_styles(\n",
        "        [{\"selector\": \"caption\", \"props\": [(\"caption-side\", \"top\"), (\"font-size\", \"16px\"), (\"font-weight\", \"bold\")]}]\n",
        "    ) \\\n",
        "    .hide(axis=\"index\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "Aa6tS0vFustC",
        "outputId": "e7fe6f35-cda3-4fc3-a1ca-41d6e2247465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78024e53da00>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_fda2f caption {\n",
              "  caption-side: top;\n",
              "  font-size: 16px;\n",
              "  font-weight: bold;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_fda2f\" class=\"dataframe\">\n",
              "  <caption>HPC Node States (from sinfo)</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_fda2f_level0_col0\" class=\"col_heading level0 col0\" >STATE</th>\n",
              "      <th id=\"T_fda2f_level0_col1\" class=\"col_heading level0 col1\" >Meaning</th>\n",
              "      <th id=\"T_fda2f_level0_col2\" class=\"col_heading level0 col2\" >Indicator</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row0_col0\" class=\"data row0 col0\" >idle</td>\n",
              "      <td id=\"T_fda2f_row0_col1\" class=\"data row0 col1\" >The node is free and ready to run a job.</td>\n",
              "      <td id=\"T_fda2f_row0_col2\" class=\"data row0 col2\" >✅ Available</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row1_col0\" class=\"data row1 col0\" >alloc</td>\n",
              "      <td id=\"T_fda2f_row1_col1\" class=\"data row1 col1\" >The node is fully allocated — all resources are in use.</td>\n",
              "      <td id=\"T_fda2f_row1_col2\" class=\"data row1 col2\" >🚫 Busy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row2_col0\" class=\"data row2 col0\" >mix</td>\n",
              "      <td id=\"T_fda2f_row2_col1\" class=\"data row2 col1\" >The node is partially allocated — some resources are still available.</td>\n",
              "      <td id=\"T_fda2f_row2_col2\" class=\"data row2 col2\" >⚙️ Partially Used</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row3_col0\" class=\"data row3 col0\" >down</td>\n",
              "      <td id=\"T_fda2f_row3_col1\" class=\"data row3 col1\" >The node is offline (e.g., under maintenance or error).</td>\n",
              "      <td id=\"T_fda2f_row3_col2\" class=\"data row3 col2\" >🛠️ Unavailable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row4_col0\" class=\"data row4 col0\" >drain</td>\n",
              "      <td id=\"T_fda2f_row4_col1\" class=\"data row4 col1\" >The node is being drained of jobs and not accepting new ones.</td>\n",
              "      <td id=\"T_fda2f_row4_col2\" class=\"data row4 col2\" >🧹 Draining</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_fda2f_row5_col0\" class=\"data row5 col0\" >comp</td>\n",
              "      <td id=\"T_fda2f_row5_col1\" class=\"data row5 col1\" >The node is completing its current jobs before becoming idle.</td>\n",
              "      <td id=\"T_fda2f_row5_col2\" class=\"data row5 col2\" >🕒 Finishing Up</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Slurm Batch Scripts\n",
        "\n",
        "It's preferable to submit your job **non-interactively** to the compute nodes.  \n",
        "\n",
        "SLURM is a job scheduler that manages the programs running on the compute nodes\n",
        "\n",
        "Using a **batch script**, your job is queued until resources are available and then runs automatically without further input.\n",
        "\n",
        "---\n",
        "\n",
        "### Slurm Specifications in a Batch Script\n",
        "\n",
        "At the beginning of your batch script, you specify Slurm settings by prefixing each line with `#SBATCH`. Common settings include:\n",
        "\n",
        "- **Partition**: the resource group to request\n",
        "  ```bash\n",
        "  #SBATCH --partition=cpu-preempt\n",
        "  ```\n",
        "\n",
        "- **Number of nodes** required:\n",
        "  ```bash\n",
        "  #SBATCH --nodes=2\n",
        "  ```\n",
        "- **Total number of tasks** (or tasks per node):\n",
        "  ```bash\n",
        "  #SBATCH --ntasks=32\n",
        "  #SBATCH --ntasks-per-node=16 # 16 CPUs per node\n",
        "  ```\n",
        "- **Maximum runtime** in the format Days-Hours:Minutes:Seconds:\n",
        "  ```bash\n",
        "  #SBATCH --time=1-12:00:00  # 1 day, 12 hours\n",
        "  ```\n",
        "- Email notifications (optional):\n",
        "  ```bash\n",
        "  #SBATCH --mail-type=ALL\n",
        "  #SBATCH --mail-user=your.email@utdallas.edu\n",
        "  ```\n",
        "\n",
        "For a full list of available settings, see the [sbatch documentation](https://slurm.schedmd.com/sbatch.html).\n",
        "\n",
        "\n",
        "### Example: Job with Slurm\n",
        "\n",
        "  - This example demonstrates a Slurm batch script to run a Python/R script `example.py` (`example.R`) using 16 cores on one node of the `turing` partition, with a maximum runtime of 2 hours.\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "#SBATCH -J example          # Job name\n",
        "#SBATCH -o example.out      # Output file\n",
        "#SBATCH -e example.err      # Error file\n",
        "#SBATCH -p turing            # Partition/queue\n",
        "#SBATCH -N 1                 # Number of nodes\n",
        "#SBATCH -n 16                # Total number of tasks (CPUs)\n",
        "#SBATCH -t 02:00:00          # Maximum runtime (2 HOURS)\n",
        "\n",
        "# Run the Python script using default Python\n",
        "python example.py\n",
        "# Run the R script using default R\n",
        "Rscript example.R\n",
        "```\n",
        "**Note:** Using `python example.py` assumes your desired Python is the default in your environment. If you need a specific conda environment, activate it before running the script.\n"
      ],
      "metadata": {
        "id": "aGyOI-4x-Aaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Python Environment\n",
        "\n",
        "Python programs often require a specific version of python to run\n",
        "\n",
        "- The combination of a Python interpreter, its modules, and libraries is referred to as a Python **environment**.\n",
        "- Since it's not feasible for the HPC team to install and maintain every possible Python version and package combination,\n",
        "users are encouraged to create and manage their own Python environments.\n",
        "- This can be done using Miniconda (recommended by HPC@UTD), a lightweight Python package and environment manager that provides Conda’s functionality without installing unnecessary default packages.\n",
        "\n",
        "### Before Starting\n",
        "\n",
        "To check if Conda is available on HPC resources, run:\n",
        "\n",
        "```bash\n",
        "module keyword conda\n",
        "```\n",
        "If available through the module system, load it with:\n",
        "```bash\n",
        "module load miniconda/4.12.0\n",
        "```\n",
        "\n",
        "### Configuring and Managing Conda Environments\n",
        "\n",
        "Conda environments contain Python versions, packages, and dependencies isolated from other environments. They let you customize Python for specific workflows.\n",
        "\n",
        "#### Initializing Conda\n",
        "\n",
        "To enable the `conda` command in your shell:\n",
        "\n",
        "```bash\n",
        "conda init bash\n",
        "```\n",
        "Then either log out and back in, or run:\n",
        "```bash\n",
        "source ~/.bashrc\n",
        "```\n",
        "Your prompt should show `(base)` indicating the base Conda environment is active.  \n",
        "To undo initialization:\n",
        "```bash\n",
        "conda init --reverse\n",
        "```\n",
        "#### Creating Environments\n",
        "\n",
        "Create a new environment named `myenv`:\n",
        "```bash\n",
        "conda create --name myenv\n",
        "```\n",
        "Activate it:\n",
        "```bash\n",
        "conda activate myenv\n",
        "```\n",
        "By default, new environments have no packages installed. Install Python and packages like this:\n",
        "```bash\n",
        "conda install python=3.9\n",
        "conda install numpy scipy matplotlib\n",
        "conda install -c pytorch pytorch\n",
        "```\n",
        "#### Verifying Your Environment\n",
        "\n",
        "Ensure the correct Python and pip executables are being used:\n",
        "```bash\n",
        "which python\n",
        "which pip\n",
        "python --version\n",
        "```\n",
        "You should see paths pointing to `$HOME/.conda/envs/myenv`.\n"
      ],
      "metadata": {
        "id": "Pq11Mx3a_JIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Running a Job on HPC\n",
        "\n",
        "To run a Python/R job on a HPC system, you typically need **two files**:\n",
        "\n",
        "1. **Python script (`.py`) or R script (`.R`)**  \n",
        "   - This file contains the Python or R code you want to run.\n",
        "   - Example: `example.py`, `example.R`\n",
        "\n",
        "2. **Slurm batch script (`.sh`)**  \n",
        "   - This file tells Slurm how to run your Python script on the cluster.\n",
        "   - Example: `example_python.sh`, `example_R.sh`\n",
        "\n",
        "### Example Workflow\n",
        "\n",
        "1. **Create your Python or R script**\n",
        "   ```bash\n",
        "   nano example.py\n",
        "   nano example.R"
      ],
      "metadata": {
        "id": "bd7xnLY4Aqlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example.py\n",
        "print(\"Hello from Python on HPC!\")\n",
        "x = [i**2 for i in range(5)]\n",
        "print(\"Squares:\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMc-GaG3ITIA",
        "outputId": "b15b6b02-31dd-4e66-af1b-a2c43f2be340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from Python on HPC!\n",
            "Squares: [0, 1, 4, 9, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installs the rpy2 package, which acts as a bridge between Python and R.\n",
        "!pip install rpy2\n",
        "# Loads the R extension\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xtxe7LIVQ4jb",
        "outputId": "ce483401-e725-42c1-bf1f-e8c11c1ee556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rpy2 in /usr/local/lib/python3.12/dist-packages (3.5.17)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from rpy2) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from rpy2) (3.1.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.12/dist-packages (from rpy2) (5.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.15.1->rpy2) (2.23)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->rpy2) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# example.R\n",
        "cat(\"Hello from R on HPC!\\n\")\n",
        "x <- 1:5\n",
        "cat(\"Squares:\", x^2, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqSzhGFLIODJ",
        "outputId": "c8b56171-2631-4ad5-94fe-5d6578f9f280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from R on HPC!\n",
            "Squares: 1 4 9 16 25 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Create your Slurm batch script**\n",
        "\n",
        "    ```bash\n",
        "    nano example_python.sh\n",
        "    ```\n",
        "\n",
        "    ```bash\n",
        "    #!/bin/bash\n",
        "    #SBATCH -J example_python\n",
        "    #SBATCH -o /home/txw200000/ParallelComputingPython/example_python.out\n",
        "    #SBATCH -e /home/txw200000/ParallelComputingPython/example_python.err\n",
        "    #SBATCH -p turing\n",
        "    #SBATCH -N 1\n",
        "    #SBATCH -n 16\n",
        "    #SBATCH -t 02:00:00\n",
        "\n",
        "    # -----------------------------\n",
        "    # Paths\n",
        "    PYTHON=/home/txw200000/.conda/envs/ml_env/bin/python\n",
        "    SCRIPT=/home/txw200000/ParallelComputingPython/example.py\n",
        "    # -----------------------------\n",
        "\n",
        "    echo \"===== Environment Info =====\"\n",
        "    echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
        "    echo \"SLURM Nodes: $SLURM_NODELIST\"\n",
        "    echo \"CPUs allocated: $SLURM_CPUS_ON_NODE\"\n",
        "    echo \"----------------------------\"\n",
        "\n",
        "    echo \"Running Python code\"\n",
        "    $PYTHON $SCRIPT\n",
        "    ```\n",
        "\n",
        "    ```bash\n",
        "    nano example_R.sh\n",
        "    ```\n",
        "\n",
        "    ```bash\n",
        "    #!/bin/bash         \n",
        "    #SBATCH -J example_R\n",
        "    #SBATCH -o /home/txw200000/ParallelComputingPython/example_R.out\n",
        "    #SBATCH -e /home/txw200000/ParallelComputingPython/example_R.err\n",
        "    #SBATCH -p turing\n",
        "    #SBATCH -N 1\n",
        "    #SBATCH -n 16\n",
        "    #SBATCH -t 02:00:00\n",
        "\n",
        "    # -----------------------------\n",
        "    # Paths\n",
        "    R_SCRIPT=/home/txw200000/ParallelComputingPython/example.R\n",
        "    # -----------------------------\n",
        "\n",
        "    echo \"===== Environment Info =====\"\n",
        "    echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
        "    echo \"SLURM Nodes: $SLURM_NODELIST\"\n",
        "    echo \"CPUs allocated: $SLURM_CPUS_ON_NODE\"\n",
        "    echo \"----------------------------\"\n",
        "\n",
        "    echo \"Running R code\"\n",
        "\n",
        "    module swap gnu12 gnu9/9.4.0\n",
        "    module load R/4.4.1\n",
        "\n",
        "    Rscript $R_SCRIPT\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "1eTkXnfXIxf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Submitting the Job**\n",
        "\n",
        "    Use the following command to submit your batch script to Slurm:\n",
        "\n",
        "    ```bash\n",
        "    sbatch ~/ParallelComputingPython/example_python.sh\n",
        "    sbatch ~/ParallelComputingPython/example_R.sh\n",
        "    ```\n",
        "\n",
        "    - Slurm will queue your job and run it when resources are available.  \n",
        "    - Output and errors will be saved in `example_python.out` (`example_R.out`) and `example_python.err` (`example_R.err`) respectively.\n",
        "\n",
        "    ```text\n",
        "    Python/R script (.py/.R)\n",
        "        │\n",
        "        ▼\n",
        "    Slurm batch script (.sh)\n",
        "        │\n",
        "        ▼\n",
        "    sbatch command\n",
        "        │\n",
        "        ▼\n",
        "    Job queued in Slurm\n",
        "        │\n",
        "        ▼\n",
        "    Job runs on compute nodes\n",
        "        │\n",
        "        ▼\n",
        "    Output saved in .out / .err files\n",
        "    ```"
      ],
      "metadata": {
        "id": "izaPpuJpJt18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Check HPC output**\n",
        "\n",
        "    ```bash\n",
        "    ===== Environment Info =====\n",
        "    SLURM Job ID: 2573603\n",
        "    SLURM Nodes: g-01-07\n",
        "    CPUs allocated: 16\n",
        "    ----------------------------\n",
        "    Running Python code\n",
        "    Hello from Python on HPC!\n",
        "    Squares: [0, 1, 4, 9, 16]\n",
        "    ```\n",
        "\n",
        "    ```bash\n",
        "    ===== Environment Info =====\n",
        "    SLURM Job ID: 2573601\n",
        "    SLURM Nodes: g-01-07\n",
        "    CPUs allocated: 16\n",
        "    ----------------------------\n",
        "    Running R code\n",
        "    Hello from R on HPC!\n",
        "    Squares: 1 4 9 16 25\n",
        "    ```"
      ],
      "metadata": {
        "id": "yxVkC-ihPwHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Parallel Computing\n",
        "\n",
        "\n",
        "1.  One computer (node), multiple cores: Multiple processors work simultaneously to speed up tasks.\n",
        "2.  Multiple computers (nodes): Networked computers collaborate to solve large problems faster."
      ],
      "metadata": {
        "id": "NFh874sLD0OH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One computer (node), multiple cores"
      ],
      "metadata": {
        "id": "0yQeEFHkK9Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Python"
      ],
      "metadata": {
        "id": "hUijoTFFLzZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from multiprocessing import Pool, cpu_count\n",
        "\n",
        "# Simulate one replicate: mean of sum of two normals.\n",
        "def simulate(_):\n",
        "    x = np.random.normal(0, 1, 100)\n",
        "    y = np.random.normal(5, 1, 100)\n",
        "    sums = x + y\n",
        "    return np.mean(sums)\n",
        "\n",
        "# Sequential run 100000 simulations\n",
        "def run_sequential(n_iter=100000):\n",
        "    start = time.time()\n",
        "    results = [simulate(i) for i in range(n_iter)]\n",
        "    mean_val = np.mean(results)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"Sequential mean: {mean_val:.4f}, time: {elapsed:.4f} seconds\")\n",
        "    return elapsed\n",
        "\n",
        "# Parallel run 100000 simulations\n",
        "def run_parallel(n_iter=100000):\n",
        "    start = time.time()\n",
        "    n_cpus = int(os.environ.get(\"SLURM_CPUS_ON_NODE\", os.cpu_count()))\n",
        "    with Pool(n_cpus) as pool:\n",
        "        results = pool.map(simulate, range(n_iter))\n",
        "    print(f\"Number of CUPs: {n_cpus}\")\n",
        "    mean_val = np.mean(results)\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"Parallel mean: {mean_val:.4f}, time: {elapsed:.4f} seconds\")\n",
        "    return elapsed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    n_iter = 100000\n",
        "    print(\"=== Simulation of Normal Sums ===\")\n",
        "    time_seq = run_sequential(n_iter)\n",
        "    time_par = run_parallel(n_iter)\n",
        "\n",
        "    # Speedup calculation\n",
        "    print(f\"Speedup: {time_seq/time_par:.2f}x\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev9i5FJ0Ki2F",
        "outputId": "78c761a6-7db3-41be-8193-0d9ad18ace28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simulation of Normal Sums ===\n",
            "Sequential mean: 5.0001, time: 1.7886 seconds\n",
            "Number of CUPs: 2\n",
            "Parallel mean: 5.0005, time: 1.9126 seconds\n",
            "Speedup: 0.94x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "#!/bin/bash\n",
        "#SBATCH -J sim_normal\n",
        "#SBATCH -o /home/txw200000/ParallelComputingPython/sim_normal.out\n",
        "#SBATCH -e /home/txw200000/ParallelComputingPython/sim_normal.err\n",
        "#SBATCH -p turing\n",
        "#SBATCH -N 1\n",
        "#SBATCH -n 16\n",
        "#SBATCH -t 02:00:00\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "PYTHON=/home/txw200000/.conda/envs/ml_env/bin/python\n",
        "SCRIPT=/home/txw200000/ParallelComputingPython/sim_normal.py\n",
        "# -----------------------------\n",
        "\n",
        "echo \"===== Environment Info =====\"\n",
        "echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
        "echo \"SLURM Nodes: $SLURM_NODELIST\"\n",
        "echo \"CPUs allocated: $SLURM_CPUS_ON_NODE\"\n",
        "echo \"Using Python: $($PYTHON -c 'import sys; print(sys.executable)')\"\n",
        "echo \"----------------------------\"\n",
        "\n",
        "echo \"Running Simulation: Sequential vs Parallel\"\n",
        "$PYTHON $SCRIPT\n",
        "```"
      ],
      "metadata": {
        "id": "YILT_IKcLfm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### HPC Output\n",
        "\n",
        "```bash\n",
        "===== Environment Info =====\n",
        "SLURM Job ID: 2540987\n",
        "SLURM Nodes: g-01-07\n",
        "CPUs allocated: 16\n",
        "Using Python: /home/txw200000/.conda/envs/ml_env/bin/python\n",
        "----------------------------\n",
        "Running Simulation: Sequential vs Parallel\n",
        "=== Simulation of Normal Sums ===\n",
        "Sequential mean: 4.9997, time: 1.4248 seconds\n",
        "Parallel mean:   5.0000, time: 0.5081 seconds\n",
        "Speedup:    2.80x\n",
        "```"
      ],
      "metadata": {
        "id": "3JF1g2hhLjmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### R"
      ],
      "metadata": {
        "id": "Dntj795-L2Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%R\n",
        "# Load the 'parallel' package for multicore computation\n",
        "library(parallel)\n",
        "\n",
        "# Simulate one replicate: mean of sum of two normals.\n",
        "simulate_one <- function(i) {\n",
        "  x <- rnorm(100, 0, 1)\n",
        "  y <- rnorm(100, 5, 1)\n",
        "  mean(x + y)\n",
        "}\n",
        "\n",
        "# Sequential run 100000 simulations\n",
        "run_sequential <- function(n_iter = 100000) {\n",
        "  start <- Sys.time()\n",
        "  results <- sapply(1:n_iter, simulate_one)\n",
        "  mean_val <- mean(results)\n",
        "  end <- Sys.time()\n",
        "  elapsed <- as.numeric(difftime(end, start, units = \"secs\"))\n",
        "  cat(sprintf(\"Sequential mean: %.4f, time: %.4f seconds\\n\", mean_val, elapsed))\n",
        "  return(elapsed)\n",
        "}\n",
        "\n",
        "# Parallel run 100000 simulations\n",
        "run_parallel <- function(n_iter = 100000) {\n",
        "  # Use SLURM allocated CPUs\n",
        "  n_cores <- as.numeric(Sys.getenv(\"SLURM_CPUS_ON_NODE\", 2))\n",
        "  print(sprintf(\"Using %d CPUs\", n_cores))\n",
        "  # For Linux nodes, FORK type is faster\n",
        "  cl <- makeCluster(n_cores, type = \"FORK\")\n",
        "  # Export function to workers\n",
        "  clusterExport(cl, \"simulate_one\")\n",
        "  start <- Sys.time()\n",
        "  results <- parSapply(cl, 1:n_iter, simulate_one)\n",
        "  mean_val <- mean(results)\n",
        "  end <- Sys.time()\n",
        "  stopCluster(cl)\n",
        "  elapsed <- as.numeric(difftime(end, start, units = \"secs\"))\n",
        "  cat(sprintf(\"Parallel mean:   %.4f, time: %.4f seconds\\n\", mean_val, elapsed))\n",
        "  return(elapsed)\n",
        "}\n",
        "\n",
        "# Main\n",
        "n_iter <- 100000\n",
        "cat(\"=== Simulation of Normal Sums ===\\n\")\n",
        "time_seq <- run_sequential(n_iter)\n",
        "time_par <- run_parallel(n_iter)\n",
        "\n",
        "# Speedup\n",
        "cat(sprintf(\"Speedup: %.2fx\\n\", time_seq / time_par))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Qa2N_LJL4DS",
        "outputId": "b836c803-e97d-4f08-e249-fd7dee4fe658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Simulation of Normal Sums ===\n",
            "Sequential mean: 5.0003, time: 2.5477 seconds\n",
            "[1] \"Using 2 CPUs\"\n",
            "Parallel mean:   4.9999, time: 2.2929 seconds\n",
            "Speedup: 1.11x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "#!/bin/bash\n",
        "#SBATCH -J sim_normal_R\n",
        "#SBATCH -o /home/txw200000/ParallelComputingPython/sim_normal_R.out\n",
        "#SBATCH -e /home/txw200000/ParallelComputingPython/sim_normal_R.err\n",
        "#SBATCH -p turing\n",
        "#SBATCH -N 1\n",
        "#SBATCH -n 16\n",
        "#SBATCH -t 02:00:00\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "R_SCRIPT=/home/txw200000/ParallelComputingPython/sim_normal.R\n",
        "# -----------------------------\n",
        "\n",
        "echo \"===== Environment Info =====\"\n",
        "echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
        "echo \"SLURM Nodes: $SLURM_NODELIST\"\n",
        "echo \"CPUs allocated: $SLURM_CPUS_ON_NODE\"\n",
        "echo \"----------------------------\"\n",
        "\n",
        "echo \"Running Simulation: Sequential vs Parallel\"\n",
        "\n",
        "module swap gnu12 gnu9/9.4.0\n",
        "module load R/4.4.1\n",
        "\n",
        "Rscript $R_SCRIPT\n"
      ],
      "metadata": {
        "id": "DG-nc_UxL-dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### HPC Output\n",
        "```bash\n",
        "===== Environment Info =====\n",
        "SLURM Job ID: 2540988\n",
        "SLURM Nodes: g-01-07\n",
        "CPUs allocated: 16\n",
        "R version: [1] \"R version 4.4.1 (2024-06-14)\"\n",
        "----------------------------\n",
        "Running Simulation: Sequential vs Parallel\n",
        "=== Simulation of Normal Sums ===\n",
        "Sequential mean: 5.0000, time: 1.2385 seconds\n",
        "Parallel mean:   5.0002, time: 0.3258 seconds\n",
        "Speedup: 3.80x\n"
      ],
      "metadata": {
        "id": "H2mDRXZJMFTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple computers (nodes)"
      ],
      "metadata": {
        "id": "N_Z5rGkaMRpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkmrq0nrR1Is",
        "outputId": "5a5144ed-90ff-46c7-cada-a6bc8016c427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.1.0-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n",
            "Downloading mpi4py-4.1.0-cp312-cp312-manylinux1_x86_64.manylinux_2_5_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --- Fixed parameters ---\n",
        "N_ITER = 100000\n",
        "SAMPLE_SIZE = 100\n",
        "\n",
        "# Simulate one replicate\n",
        "def simulate_one():\n",
        "    x = np.random.normal(0, 1, SAMPLE_SIZE)\n",
        "    y = np.random.normal(5, 1, SAMPLE_SIZE)\n",
        "    return np.mean(x + y)\n",
        "\n",
        "# Simulate a chunk of replicates\n",
        "def simulate_chunk(n_iter):\n",
        "    return [simulate_one() for _ in range(n_iter)]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the MPI communicator\n",
        "    comm = MPI.COMM_WORLD\n",
        "    # Rank (ID) of this process, from 0 to size-1\n",
        "    rank = comm.Get_rank()\n",
        "    # Total number of MPI processes in this communicator\n",
        "    size = comm.Get_size()\n",
        "\n",
        "    if rank == 0:\n",
        "        print(f\"Total MPI tasks: {size}\")\n",
        "        print(f\"Total simulations: {N_ITER}\")\n",
        "        print(f\"Sample size per replicate: {SAMPLE_SIZE}\")\n",
        "\n",
        "        # Sequential run (optional)\n",
        "        start_seq = time.time()\n",
        "        seq_results = simulate_chunk(N_ITER)\n",
        "        mean_seq = np.mean(seq_results)\n",
        "        time_seq = time.time() - start_seq\n",
        "        print(f\"[Sequential] Mean: {mean_seq:.4f}, time: {time_seq:.4f} s\")\n",
        "\n",
        "    # Parallel run\n",
        "    n_iter_per_task = N_ITER // size\n",
        "    # Synchronize all MPI processes\n",
        "    comm.Barrier()\n",
        "    start_par = time.time()\n",
        "\n",
        "    local_results = simulate_chunk(n_iter_per_task)\n",
        "    all_results = comm.gather(local_results, root=0)\n",
        "\n",
        "    end_par = time.time()\n",
        "\n",
        "    if rank == 0:\n",
        "        all_results = np.concatenate(all_results)\n",
        "        mean_par = np.mean(all_results)\n",
        "        time_par = end_par - start_par\n",
        "        print(f\"[Parallel] Mean: {mean_par:.4f}, time: {time_par:.4f} s\")\n",
        "        print(f\"Total replicates collected: {len(all_results)}\")\n",
        "        print(f\"Speedup: {time_seq / time_par:.2f}x\")"
      ],
      "metadata": {
        "id": "A-3Se1rYSS5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "#!/bin/bash\n",
        "#SBATCH -J sim_normal_mpi\n",
        "#SBATCH -o /home/txw200000/ParallelComputingPython/sim_normal_mpi.out\n",
        "#SBATCH -e /home/txw200000/ParallelComputingPython/sim_normal_mpi.err\n",
        "#SBATCH -p turing\n",
        "#SBATCH -N 2\n",
        "#SBATCH -n 20\n",
        "#SBATCH --ntasks-per-node=10\n",
        "#SBATCH -t 02:00:00\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "PYTHON=/home/txw200000/.conda/envs/ml_env/bin/python\n",
        "SCRIPT=/home/txw200000/ParallelComputingPython/sim_normal_mpi.py\n",
        "# -----------------------------\n",
        "\n",
        "echo \"===== Environment Info =====\"\n",
        "echo \"SLURM Job ID: $SLURM_JOB_ID\"\n",
        "echo \"SLURM Nodes: $SLURM_NODELIST\"\n",
        "echo \"MPI tasks:   $SLURM_NTASKS\"\n",
        "echo \"Using Python: $($PYTHON -c 'import sys; print(sys.executable)')\"\n",
        "echo \"----------------------------\"\n",
        "\n",
        "echo \"Running MPI bootstrap simulation\"\n",
        "prun $PYTHON $SCRIPT\n",
        "\n"
      ],
      "metadata": {
        "id": "LgrANKRQNCrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### HPC Output\n",
        "```bash\n",
        "===== Environment Info =====\n",
        "SLURM Job ID: 2581721\n",
        "SLURM Nodes: c-05-12,g-01-07\n",
        "MPI tasks:   20\n",
        "Using Python: /home/txw200000/.conda/envs/ml_env/bin/python\n",
        "----------------------------\n",
        "Running MPI bootstrap simulation\n",
        "[prun] Master compute host = c-05-12\n",
        "[prun] Resource manager = slurm\n",
        "[prun] Launch cmd = mpirun /home/txw200000/.conda/envs/ml_env/bin/python /home/txw200000/ParallelComputingPython/sim_normal_mpi.py (family=openmpi4)\n",
        "Total MPI tasks: 20\n",
        "Total simulations: 100000\n",
        "Sample size per replicate: 100\n",
        "[Sequential] Mean: 4.9999, time: 1.6111 s\n",
        "[Parallel] Mean: 4.9995, time: 0.2374 s\n",
        "Total replicates collected: 100000\n",
        "Speedup: 6.79x\n"
      ],
      "metadata": {
        "id": "lor-QW7xNLTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Getting Help\n",
        "\n",
        "- Visit the [HPC@UTD services page](https://hpc.utdallas.edu/services/)\n",
        "\n",
        "- Email circ-assist@utdallas.edu\n",
        "\n",
        "- Stop by HPC@UTD office on weekdays between 9:30 am and 3:30 pm in the Administration Building, AD 3.207"
      ],
      "metadata": {
        "id": "vds-OseK1_8r"
      }
    }
  ]
}